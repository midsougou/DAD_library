{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <b>Final Project</b>\n",
    "    <br>\n",
    "    Jarry Guillaume\n",
    "    <br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import kmedoids \n",
    "import os \n",
    "import glob\n",
    "from sklearn.svm import OneClassSVM\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from BenchMark.markov_dataset_generator import MarkovDatasetGenerator\n",
    "from BenchMark.markov_sequence import MarkovSequenceGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the categorical Datasets from the ADRepository-Anomaly-detection-datasets github repository. It is available here : \n",
    "\n",
    "- https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets?tab=readme-ov-fil\n",
    "\n",
    "Since our article is focused on aonmaly detection for discrete timeseries, these dataset will allow us to deploy some of the techniques showcased in the article. Let's start ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"ADRepository-Anomaly-detection-datasets/categorical data/\"\n",
    "datasets = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "    try: \n",
    "        data, meta = arff.loadarff(filepath)\n",
    "        datasets.append((data, meta))\n",
    "    except: \n",
    "        print(f\"Error while parsing file : {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Generating Synthetic Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data we tried to find online for synthetic timeseries was rarely labeled, we propose to generate some synthetic data so that we can test and implement as many algorithm for our discreet anomaly detection library. We can then test our algorithm on some real, less labeled data.\n",
    "\n",
    "### Markovian models :     \n",
    "\n",
    "This class will generate synthetic data that creates Markovian Discreet sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us generate a dataset mixing a hidden Markow model and two markow models and let us wrap them up into the same dataset. We will also add an anomaly dataset, which will be another Markov model, with different probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7]\n",
    "])\n",
    "\n",
    "hidden_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.2, 0.6]\n",
    "])\n",
    "\n",
    "transition_matrix2 = np.array([\n",
    "    [0.6, 0.3, 0.1],  # Emission distribution from hidden state 0\n",
    "    [0.2, 0.5, 0.3],  # Emission distribution from hidden state 1\n",
    "    [0.1, 0.2, 0.7]   # Emission distribution from hidden state 2\n",
    "])\n",
    "\n",
    "N = 5  \n",
    "transition_matrix3 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix3.sum(axis=1, keepdims=True)\n",
    "transition_matrix3 = transition_matrix3 / row_sums\n",
    "\n",
    "transition_matrices = [transition_matrix1, transition_matrix2, transition_matrix3]\n",
    "hidden_matrices = [hidden_matrix1, None, None ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MarkovDatasetGenerator(transition_matrices, hidden_matrices, n_sequences=200, sequence_length=50)\n",
    "train_dataset = generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "transition_matrix4 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix4.sum(axis=1, keepdims=True)\n",
    "transition_matrix4 = transition_matrix4 / row_sums\n",
    "test_generator = MarkovDatasetGenerator([transition_matrix4, transition_matrix3], [None, None], n_sequences=20, sequence_length=50)\n",
    "test_dataset = test_generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Kernell-Based Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernellBase: \n",
    "    def __init__(self, dataset, similarity_metric): \n",
    "        self.dataset = dataset\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_matrix = None\n",
    "        self.medoids = None\n",
    "\n",
    "    def compute_similarity_matrix(self): \n",
    "        n = len(self.dataset)\n",
    "        self.similarity_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                sim = self.similarity_metric(self.dataset[i], self.dataset[j])\n",
    "                self.similarity_matrix[i,j] = sim\n",
    "                self.similarity_matrix[j,i] = sim  # Symmetric\n",
    "        \n",
    "        self.distance_matrix = 1 - self.similarity_matrix\n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def compute_kemedoids(self, kmax=10, kmin=1): \n",
    "        km = kmedoids.dynmsc(self.distance_matrix, kmax, kmin)\n",
    "        self.medoids = [self.dataset[medoid] for medoid in km.medoids]\n",
    "        return self.medoids\n",
    "\n",
    "    def knearest_predict(self, test_sequence, k_nearest=5):\n",
    "        similarities = []\n",
    "        for sequence in self.dataset:  \n",
    "            similarities.append(self.similarity_metric(test_sequence, sequence))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        anomaly_score = 1 / similarities[k_nearest]\n",
    "        return anomaly_score\n",
    "\n",
    "    def clustering_predict(self, test_sequence, kmax=10, kmin=1):\n",
    "        if self.similarity_matrix is None: \n",
    "            self.compute_similarity_matrix()\n",
    "        if self.medoids is None:\n",
    "            self.compute_kemedoids(kmax=kmax, kmin=kmin)\n",
    "\n",
    "        max_similarity = 0\n",
    "        for medoid in self.medoids: \n",
    "            max_similarity = max(max_similarity, self.similarity_metric(test_sequence, medoid))\n",
    "        \n",
    "        return 1 / max_similarity       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try our Kernell based methods with the longest common sequence kernell suggested in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS_length(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = [[0]*(len2+1) for _ in range(len1+1)]\n",
    "    for i in range(1, len1+1):\n",
    "        for j in range(1, len2+1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[len1][len2]\n",
    "\n",
    "def nLCS(seq1, seq2):\n",
    "    lcs = LCS_length(seq1, seq2)\n",
    "    return lcs / ( (len(seq1)*len(seq2))**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernell_based = KernellBase(train_dataset, nLCS)\n",
    "distance_matrix = kernell_based.compute_similarity_matrix()\n",
    "medoids = kernell_based.compute_kemedoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Window Based Techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Based Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowBasedStruct: \n",
    "    def __init__(self, dataset, window_length, mode=\"average\"): \n",
    "        self.window_length = window_length\n",
    "        self.dataset = self.partition(dataset)\n",
    "        self.mode = self.mode\n",
    "\n",
    "    def partition(self, dataset): \n",
    "        partition = []\n",
    "        for sequence in dataset: \n",
    "            partition = [sequence[i*self.window_length:(i+1)*self.window_length] for i in range(int(len(sequence) // self.window_length))]\n",
    "            partition.append(partition)\n",
    "        \n",
    "        return partition\n",
    "\n",
    "    def process_anomaly(self, anomaly_scores): \n",
    "        if self.mode == \"average\": \n",
    "            return np.mean(anomaly_scores)\n",
    "        elif self.mode == \"max\": \n",
    "            return np.max(anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Lookahead(WindowBasedStruct): \n",
    "    def __init__(self, dataset, window_length, mode=\"average\", k_look_ahead=5): \n",
    "        super().__init__(dataset, window_length, mode=mode)\n",
    "        self.k_look_ahead = k_look_ahead\n",
    "    \n",
    "    def train(self): \n",
    "        self.lookahead_dict = {}\n",
    "        for partition in self.dataset:\n",
    "            for seq in partition: \n",
    "                for i in range(len(seq) - self.k_look_ahead):\n",
    "                    pair = (seq[i], seq[i + self.k_look_ahead])\n",
    "                    self.lookahead_dict[pair] = self.lookahead_dict.get(pair, 0) + 1\n",
    "        return self.lookahead_dict\n",
    "\n",
    "    def predict(self, test_dataset): \n",
    "        test_dataset = self.partition(test_dataset)\n",
    "\n",
    "        anomaly_score = []\n",
    "        for partition in test_dataset: \n",
    "            for sequence in partition:\n",
    "                anomalies = []\n",
    "                for i in range(len(sequence) - self.k_look_ahead):\n",
    "                    pair = (sequence[i], sequence[i + self.k_look_ahead])\n",
    "                    anomalies.append(self.lookahead_dict.get(pair, 0))\n",
    "\n",
    "            anomaly_score.append()\n",
    "\n",
    "        return anomaly_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class UnsupervisedSVM(WindowBasedStruct): \n",
    "    def __init__(self, dataset, window_length, mode=\"average\"):\n",
    "        super().__init__(dataset, window_length, mode=mode)\n",
    "        self.dataset = self.one_hot_encoding(dataset)\n",
    "\n",
    "    def one_hot_encoding(self): \n",
    "        new_dataset = []\n",
    "        for partition in self.dataset:\n",
    "            one_hot_encoded = []\n",
    "            for seq in partition:\n",
    "                mat = np.zeros((self.n_symbols, len(seq)), dtype=int)\n",
    "\n",
    "                for pos, symbol in enumerate(seq):\n",
    "                    s_idx = self.symbol_to_idx[symbol]\n",
    "                    mat[s_idx, pos] = 1\n",
    "\n",
    "                one_hot_encoded.append(mat)\n",
    "            new_dataset.append(one_hot_encoded)\n",
    "        return new_dataset\n",
    "    \n",
    "    def train(self): \n",
    "        self.svm = OneClassSVM(gamma='auto').fit(self.dataset)\n",
    "        return self.svm\n",
    "    \n",
    "    def predict(self, test): \n",
    "        test = self.partition(test)\n",
    "        test = self.one_hot_encoding(test)\n",
    "        return self.svm.predict(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Dictionary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDictionary(WindowBasedStruct): \n",
    "    def __init__(self, dataset, window_length, mode=\"average\"): \n",
    "        super().__init__(dataset, window_length, mode=mode)\n",
    "\n",
    "    def train(self): \n",
    "        self.frequency_dictionary = {}\n",
    "        for partition in self.dataset: \n",
    "            for sequence in partition: \n",
    "                sequence = tuple(sequence)\n",
    "                if sequence in self.frequency_dictionary.keys():\n",
    "                    self.frequency_dictionary[tuple(sequence)] = self.frequency_dictionary.get(sequence) + 1\n",
    "\n",
    "        return self.frequency_dictionary\n",
    "\n",
    "    def predict(self, test_dataset):\n",
    "        test_dataset = self.partition(test_dataset) \n",
    "\n",
    "        anomaly_scores = []\n",
    "        for partition in test_dataset: \n",
    "            anomalies = []\n",
    "            for sequence in partition: \n",
    "                anomalies.append(self.frequency_dictionary.get(tuple(sequence)))\n",
    "            \n",
    "            score = self.process_anomaly(anomalies)\n",
    "            anomaly_scores.append(score)\n",
    "\n",
    "        return anomaly_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-side Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSIDE(WindowBasedStruct): \n",
    "    def __init__(self, dataset, window_length, mode=\"average\"): \n",
    "        super().__init__(dataset, window_length, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) Hidden Markov Models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "class ViterbiTrainerHMM:\n",
    "    def __init__(self, n_states, n_symbols, max_iter=50, tol=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_symbols = n_symbols\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "        A = np.random.rand(n_states, n_states)\n",
    "        self.A = A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "        B = np.random.rand(n_states, n_symbols)\n",
    "        self.B = B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.pi = np.full(n_states, 1.0/n_states)\n",
    "        # to map the alphabets into numbers\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.encoder.fit(list(ALPHABET[:n_symbols]))\n",
    "\n",
    "    def _encode(self, sequence):\n",
    "        return self.encoder.transform(sequence)\n",
    "    \n",
    "    def _viterbi(self, O):\n",
    "        \"\"\"\n",
    "        Viterbi algorithm for a single sequence O:\n",
    "        O: array of shape (T,) containing integer-coded observations\n",
    "        returns:\n",
    "          states: most likely state sequence\n",
    "          prob: probability of that sequence under the model\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        delta = np.zeros((T, self.n_states))\n",
    "        psi = np.zeros((T, self.n_states), dtype=int)\n",
    "        O = self._encode(O)\n",
    "        # Initialization\n",
    "        delta[0, :] = self.pi * self.B[:, O[0]]\n",
    "        psi[0, :] = 0\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                vals = delta[t-1, :] * self.A[:, j]\n",
    "                i_star = np.argmax(vals)\n",
    "                delta[t, j] = vals[i_star] * self.B[j, O[t]]\n",
    "                psi[t, j] = i_star\n",
    "\n",
    "        # Termination\n",
    "        p_star = np.max(delta[T-1, :])\n",
    "        q_star = np.zeros(T, dtype=int)\n",
    "        q_star[T-1] = np.argmax(delta[T-1, :])\n",
    "\n",
    "        # Backtrack\n",
    "        for t in range(T-2, -1, -1):\n",
    "            q_star[t] = psi[t+1, q_star[t+1]]\n",
    "\n",
    "        return q_star, p_star\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"\n",
    "        Fit the HMM parameters to the dataset using Viterbi training.\n",
    "        dataset: list of sequences, each sequence is a list or array of integer-coded observations.\n",
    "        \"\"\"\n",
    "        prev_log_likelihood = -np.inf\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Accumulators for counts\n",
    "            pi_counts = np.zeros(self.n_states)\n",
    "            A_counts = np.zeros((self.n_states, self.n_states))\n",
    "            B_counts = np.zeros((self.n_states, self.n_symbols))\n",
    "\n",
    "            total_log_likelihood = 0.0\n",
    "\n",
    "            # For each sequence, run Viterbi to get most likely path\n",
    "            for O in dataset:\n",
    "                states, seq_prob = self._viterbi(O)\n",
    "                O = self._encode(O)\n",
    "                total_log_likelihood += np.log(seq_prob + 1e-12)\n",
    "\n",
    "                # Update counts based on the Viterbi path\n",
    "                pi_counts[states[0]] += 1\n",
    "                for t in range(len(O)):\n",
    "                    B_counts[states[t], O[t]] += 1\n",
    "                    if t > 0:\n",
    "                        A_counts[states[t-1], states[t]] += 1\n",
    "\n",
    "            # Normalize to get new parameters\n",
    "            self.pi = pi_counts / pi_counts.sum() if pi_counts.sum() > 0 else self.pi\n",
    "\n",
    "            row_sums_A = A_counts.sum(axis=1, keepdims=True)\n",
    "            row_sums_A[row_sums_A == 0] = 1e-12\n",
    "            self.A = A_counts / row_sums_A\n",
    "\n",
    "            row_sums_B = B_counts.sum(axis=1, keepdims=True)\n",
    "            row_sums_B[row_sums_B == 0] = 1e-12\n",
    "            self.B = B_counts / row_sums_B\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.abs(total_log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "            prev_log_likelihood = total_log_likelihood\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"\n",
    "        Predict the most likely state sequences for a batch of sequences.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for O in dataset:\n",
    "            states, _ = self._viterbi(O)\n",
    "            predictions.append(states)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, dataset):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood of the dataset under the current model parameters using Viterbi best paths.\n",
    "        This is not the full likelihood (like forward algorithm gives), but the likelihood of the Viterbi path.\n",
    "        \"\"\"\n",
    "        total_log_likelihood = 0.0\n",
    "        for O in dataset:\n",
    "            _, seq_prob = self._viterbi(O)\n",
    "            total_log_likelihood += np.log(seq_prob + 1e-12)\n",
    "        return total_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaumWelchHMM: \n",
    "    def __init__(self, n_symbols, n_states, max_iter=100, tolerance=1e-3):\n",
    "        self.n_symbols = n_symbols\n",
    "        self.n_states = n_states\n",
    "        self.symbols = {elt:i for i, elt in enumerate(ALPHABET[:n_symbols])}\n",
    "\n",
    "        self.tolerance = tolerance \n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        A = np.random.rand(n_states, n_states)\n",
    "        self.transition_matrix = A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "        B = np.random.rand(n_states, n_symbols)\n",
    "        self.emission_matrix  = B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.initial_state = np.full(self.n_states, 1/self.n_states)\n",
    "\n",
    "    def init_fraction(self): \n",
    "        self.gamma_first = np.zeros(self.n_states)\n",
    "        self.transition_numerator = np.zeros((self.n_states, self.n_states))\n",
    "        self.emission_numerator = np.zeros((self.n_states, self.n_symbols))    \n",
    "        self.transition_denominator = np.zeros((self.n_states))  \n",
    "        self.emission_denominator = np.zeros((self.n_states))  \n",
    "\n",
    "    def one_hot_encode(self, sequence):\n",
    "        one_hot_encoded = np.zeros((len(sequence), self.n_symbols), dtype=int)\n",
    "        one_hot_encoded[np.arange(len(sequence)), sequence] = 1\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward_scaled(self, sequence):\n",
    "        T = len(sequence)\n",
    "        alpha = np.zeros((T, self.n_states))\n",
    "        c = np.zeros(T)  # scaling factors\n",
    "\n",
    "        # Initialization\n",
    "        for i in range(self.n_states):\n",
    "            alpha[0, i] = self.initial_state[i] * self.emission_matrix[i, sequence[0]]\n",
    "        c[0] = 1.0 / (np.sum(alpha[0, :]) + 1e-300)\n",
    "        alpha[0, :] *= c[0]\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                alpha[t, j] = np.sum(alpha[t-1, :] * self.transition_matrix[:, j]) * self.emission_matrix[j, sequence[t]]\n",
    "            c[t] = 1.0 / (np.sum(alpha[t, :]) + 1e-300)\n",
    "            alpha[t, :] *= c[t]\n",
    "\n",
    "        return alpha, c\n",
    "\n",
    "    def backward_scaled(self, sequence, c):\n",
    "        T = len(sequence)\n",
    "        beta = np.zeros((T, self.n_states))\n",
    "        beta[T-1, :] = 1.0 * c[T-1]  \n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for i in range(self.n_states):\n",
    "                beta[t, i] = np.sum(self.transition_matrix[i, :] * self.emission_matrix[:, sequence[t+1]] * beta[t+1, :])\n",
    "            beta[t, :] *= c[t]\n",
    "\n",
    "        return beta\n",
    "\n",
    "    \n",
    "    def convert_dataset(self, dataset): \n",
    "        new_dataset = []\n",
    "        for sequence in dataset: \n",
    "            number_sequence = [self.symbols[letter] for letter in sequence]\n",
    "            new_dataset.append(number_sequence)\n",
    "        return new_dataset\n",
    "    \n",
    "    def E_step(self, sequence):\n",
    "        n = len(sequence)\n",
    "        alpha, c = self.forward_scaled(sequence)\n",
    "        beta = self.backward_scaled(sequence, c)\n",
    "\n",
    "        log_prob = -np.sum(np.log(c + 1e-300))\n",
    "        probability = np.exp(log_prob)\n",
    "\n",
    "        gamma = (alpha * beta)  \n",
    "        gamma = gamma / (np.sum(gamma, axis=1, keepdims=True) + 1e-300)\n",
    "\n",
    "        xi = np.zeros((n-1, self.n_states, self.n_states))\n",
    "        for t in range(n-1):\n",
    "            denom = np.sum(alpha[t, :] * beta[t, :]) + 1e-300\n",
    "            for i in range(self.n_states):\n",
    "                xi[t, i, :] = (alpha[t, i] * self.transition_matrix[i, :] *\n",
    "                            self.emission_matrix[:, sequence[t+1]] * beta[t+1, :]) / denom\n",
    "\n",
    "        return gamma, xi, probability\n",
    "\n",
    "\n",
    "    def M_step(self): \n",
    "        # print(self.gamma_first)\n",
    "        # print(self.transition_numerator, self.transition_denominator)\n",
    "        # print(self.emission_numerator, self.emission_denominator)\n",
    "        # print(\"=====================================\")\n",
    "        self.initial_state = self.gamma_first\n",
    "        self.transition_matrix = self.transition_numerator / (self.transition_denominator[:, np.newaxis] + 1e-12)\n",
    "        self.emission_matrix = self.emission_numerator / (self.emission_denominator[:, np.newaxis] + 1e-12)\n",
    "\n",
    "        self.transition_matrix /= self.transition_matrix.sum(axis=1)\n",
    "        self.emission_matrix /= self.emission_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    \n",
    "    def Baum_Welch(self, dataset):\n",
    "        n_sequence = len(dataset)\n",
    "        dataset = self.convert_dataset(dataset)\n",
    "        self.init_fraction()  \n",
    "        \n",
    "        log_likelihood = 0.0\n",
    "        \n",
    "        for sequence in dataset:\n",
    "            mask = self.one_hot_encode(sequence)\n",
    "\n",
    "            gamma, xi, probability = self.E_step(sequence)\n",
    "\n",
    "            self.gamma_first += gamma[0, :] / n_sequence\n",
    "\n",
    "            self.transition_numerator += np.sum(xi[:-1], axis=0)\n",
    "            self.emission_numerator += gamma.T @ mask\n",
    "            self.transition_denominator += np.sum(gamma[:-1], axis=0) \n",
    "            self.emission_denominator += np.sum(gamma, axis=0)\n",
    "\n",
    "            log_likelihood += np.log(probability + 1e-12)\n",
    "        \n",
    "        self.M_step()\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        prev_log_likelihood = -np.inf\n",
    "        log_likelihood = 0 \n",
    "\n",
    "        with tqdm(total=self.max_iter, desc=\"EM Algorithm Progress\", unit=\"step\") as pbar:\n",
    "            for i in range(self.max_iter):\n",
    "                prev_log_likelihood = log_likelihood  \n",
    "                log_likelihood = self.Baum_Welch(dataset)\n",
    "                pbar.set_postfix({\"Log-Likelihood\": log_likelihood})\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "    def predict_sample(self, sequence): \n",
    "        alpha = self.forward(sequence)\n",
    "        probability = np.sum(alpha[-1, :])\n",
    "        return -np.log(probability + 1e-12)\n",
    "    \n",
    "    def predict(self, dataset): \n",
    "        dataset = self.convert_dataset(dataset)\n",
    "        scores = []\n",
    "        for sequence in dataset:\n",
    "            scores.append(self.predict_sample(sequence))\n",
    "        return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImprovedHiddenMarkovModel:\n",
    "    def __init__(self, n_symbols, n_states, max_iter=100, tolerance=1e-4, \n",
    "                 smoothing=1e-10, random_seed=42):\n",
    "        \"\"\"\n",
    "        Improved HMM implementation with numerical stability\n",
    "        \n",
    "        Parameters:\n",
    "        - n_symbols: Number of possible observation symbols\n",
    "        - n_states: Number of hidden states\n",
    "        - max_iter: Maximum number of EM iterations\n",
    "        - tolerance: Convergence threshold for log-likelihood\n",
    "        - smoothing: Small value to prevent zero probabilities\n",
    "        - random_seed: Seed for reproducibility\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        self.n_symbols = n_symbols\n",
    "        self.n_states = n_states\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        # Initialize parameters with small smoothing\n",
    "        self.transition_matrix = self._initialize_transition_matrix()\n",
    "        self.emission_matrix = self._initialize_emission_matrix()\n",
    "        self.initial_state = self._initialize_initial_state()\n",
    "\n",
    "    def _initialize_transition_matrix(self):\n",
    "        \"\"\"Initialize transition matrix with smoothing\"\"\"\n",
    "        A = np.random.random((self.n_states, self.n_states)) + self.smoothing\n",
    "        return A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _initialize_emission_matrix(self):\n",
    "        \"\"\"Initialize emission matrix with smoothing\"\"\"\n",
    "        B = np.random.random((self.n_states, self.n_symbols)) + self.smoothing\n",
    "        return B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _initialize_initial_state(self):\n",
    "        \"\"\"Initialize initial state probabilities\"\"\"\n",
    "        pi = np.random.random(self.n_states) + self.smoothing\n",
    "        return pi / pi.sum()\n",
    "\n",
    "    def _log_forward_algorithm(self, sequence):\n",
    "        \"\"\"\n",
    "        Perform forward algorithm in log space to prevent underflow\n",
    "        \n",
    "        Returns:\n",
    "        - log_alpha: Log-space forward probabilities\n",
    "        - log_prob: Log probability of the sequence\n",
    "        \"\"\"\n",
    "        log_initial = np.log(self.initial_state + self.smoothing)\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        log_alpha = np.zeros((len(sequence), self.n_states))\n",
    "        log_alpha[0] = log_initial + log_emission[:, sequence[0]]\n",
    "        \n",
    "        for t in range(1, len(sequence)):\n",
    "            for j in range(self.n_states):\n",
    "                log_alpha[t, j] = logsumexp(\n",
    "                    log_alpha[t-1] + log_transition[:, j]\n",
    "                ) + log_emission[j, sequence[t]]\n",
    "        \n",
    "        log_prob = logsumexp(log_alpha[-1])\n",
    "        return log_alpha, log_prob\n",
    "\n",
    "    def _log_backward_algorithm(self, sequence, log_prob):\n",
    "        \"\"\"\n",
    "        Perform backward algorithm in log space\n",
    "        \n",
    "        Returns:\n",
    "        Log-space backward probabilities\n",
    "        \"\"\"\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        log_beta = np.zeros((len(sequence), self.n_states))\n",
    "        log_beta[-1] = 0.0\n",
    "\n",
    "        for t in range(len(sequence)-2, -1, -1):\n",
    "            for i in range(self.n_states):\n",
    "                log_beta[t, i] = logsumexp(\n",
    "                    log_transition[i, :] + \n",
    "                    log_emission[:, sequence[t+1]] + \n",
    "                    log_beta[t+1]\n",
    "                )\n",
    "        \n",
    "        return log_beta\n",
    "\n",
    "    def _compute_gamma_xi(self, sequence, log_alpha, log_beta, log_prob):\n",
    "        \"\"\"\n",
    "        Compute posterior probabilities gamma and xi\n",
    "        \"\"\"\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        # Compute log gamma (state posteriors)\n",
    "        log_gamma = log_alpha + log_beta - log_prob\n",
    "        gamma = np.exp(log_gamma)\n",
    "\n",
    "        # Compute log xi (transition posteriors)\n",
    "        xi = np.zeros((len(sequence)-1, self.n_states, self.n_states))\n",
    "        for t in range(len(sequence)-1):\n",
    "            for i in range(self.n_states):\n",
    "                for j in range(self.n_states):\n",
    "                    xi[t, i, j] = np.exp(\n",
    "                        log_alpha[t, i] + \n",
    "                        log_transition[i, j] + \n",
    "                        log_emission[j, sequence[t+1]] + \n",
    "                        log_beta[t+1, j] - \n",
    "                        log_prob\n",
    "                    )\n",
    "            xi[t] /= xi[t].sum()\n",
    "\n",
    "        return gamma, xi\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        \"\"\"\n",
    "        EM algorithm for parameter estimation\n",
    "        \n",
    "        Args:\n",
    "        sequences: List of sequences, where each sequence is a list of symbol indices\n",
    "        \"\"\"\n",
    "        prev_log_likelihood = -np.inf\n",
    "\n",
    "        for iteration in tqdm(range(self.max_iter), desc=\"EM Iterations\"):\n",
    "            # Accumulators for expected counts\n",
    "            total_gamma_0 = np.zeros(self.n_states)\n",
    "            total_transition = np.zeros_like(self.transition_matrix)\n",
    "            total_emission = np.zeros_like(self.emission_matrix)\n",
    "            log_likelihood = 0.0\n",
    "\n",
    "            # E-step\n",
    "            for sequence in sequences:\n",
    "                # Compute log-space forward and backward probabilities\n",
    "                log_alpha, log_prob = self._log_forward_algorithm(sequence)\n",
    "                log_beta = self._log_backward_algorithm(sequence, log_prob)\n",
    "                \n",
    "                # Compute posterior probabilities\n",
    "                gamma, xi = self._compute_gamma_xi(sequence, log_alpha, log_beta, log_prob)\n",
    "                \n",
    "                # Accumulate statistics\n",
    "                total_gamma_0 += gamma[0]\n",
    "                total_transition += xi.sum(axis=0)\n",
    "                \n",
    "                for t, sym in enumerate(sequence):\n",
    "                    total_emission[:, sym] += gamma[t]\n",
    "                \n",
    "                log_likelihood += log_prob\n",
    "\n",
    "            # M-step: Update parameters with smoothing\n",
    "            self.initial_state = total_gamma_0 / total_gamma_0.sum()\n",
    "            self.transition_matrix = (total_transition + self.smoothing) \n",
    "            self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)\n",
    "            \n",
    "            self.emission_matrix = (total_emission + self.smoothing)\n",
    "            self.emission_matrix /= self.emission_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.abs(log_likelihood - prev_log_likelihood) < self.tolerance:\n",
    "                break\n",
    "            \n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "        return self\n",
    "\n",
    "def logsumexp(x):\n",
    "    \"\"\"\n",
    "    Numerically stable log-sum-exp trick\n",
    "    \"\"\"\n",
    "    max_x = np.max(x)\n",
    "    return max_x + np.log(np.sum(np.exp(x - max_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_probability_matrix(n_rows, n_cols):\n",
    "    matrix = np.random.rand(n_rows, n_cols)  \n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "    matrix = matrix / row_sums\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = 4\n",
    "n_states = 3\n",
    "\n",
    "transition_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7], \n",
    "])\n",
    "\n",
    "emission_matrix1 = np.array([\n",
    "    [0.5, 0.2, 0.1, 0.2],\n",
    "    [0.1, 0.6, 0.1, 0.2],\n",
    "    [0.2, 0.2, 0.4, 0.2]\n",
    "])\n",
    "\n",
    "\n",
    "transition_matrix2 = generate_random_probability_matrix(n_states, n_states)\n",
    "emission_matrix2 = generate_random_probability_matrix(n_states, n_symbols)\n",
    "\n",
    "generator = MarkovSequenceGenerator(transition_matrix=transition_matrix1, \n",
    "                               emission_matrix=emission_matrix1, \n",
    "                               sequence_length=10)\n",
    "train_dataset = generator.generate_all_sequences(initial_state=0, n_sequence=50)\n",
    "\n",
    "\n",
    "dataset_generator = MarkovDatasetGenerator(transition_matrices=[transition_matrix1, transition_matrix2], \n",
    "                                                 emission_matrices=[emission_matrix1, emission_matrix2], \n",
    "                                                 sequence_length=10,)\n",
    "\n",
    "test_dataset, labels = dataset_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'D', 'C']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = ViterbiTrainerHMM(n_symbols=n_symbols, n_states=n_states, max_iter=100, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(hmm._encode(train_dataset[0])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.fit(train_dataset[:100])\n",
    "\n",
    "# train_dataset = hmm.convert_dataset(train_dataset)\n",
    "# cat = CategoricalHMM(n_components=3, algorithm='viterbi', n_iter=10)\n",
    "# cat.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_dataset = BaumWelchHMM(n_symbols, n_states=n_states, max_iter=10, tolerance=1e-3).convert_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ViterbiTrainerHMM' object has no attribute 'emission_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test_dataset = hmm.convert_dataset(test_dataset)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mhmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memission_matrix\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ViterbiTrainerHMM' object has no attribute 'emission_matrix'"
     ]
    }
   ],
   "source": [
    "# test_dataset = hmm.convert_dataset(test_dataset)\n",
    "hmm.emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = cat.predict(test_dataset)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.73355959e-04, 5.74615691e-10, 9.99426643e-01],\n",
       "       [8.77011591e-01, 1.12507443e-01, 1.04809666e-02],\n",
       "       [7.73528788e-01, 2.09231999e-01, 1.72392127e-02],\n",
       "       ...,\n",
       "       [6.56708989e-01, 7.20222327e-02, 2.71268779e-01],\n",
       "       [6.56258090e-01, 1.87903224e-01, 1.55838686e-01],\n",
       "       [5.18080705e-01, 2.36853056e-01, 2.45066239e-01]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.predict_proba(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-2900.2197740127285)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = hmm.score(test_dataset)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
